# Data Layers

Understanding how Dango organizes data across schemas.

---

## Overview

Dango organizes data into four layers, following modern analytics engineering best practices:

```
raw → staging → intermediate → marts
```

Each layer has a specific purpose and transformation logic:

| Layer | Schema | Purpose | Managed By | Materialization |
|-------|--------|---------|------------|-----------------|
| **Raw** | `raw.*` or `raw_<source>.*` | Immutable source data | dlt | Tables |
| **Staging** | `staging.*` | Cleaned, deduplicated | Auto-generated (dbt) | Views |
| **Intermediate** | `intermediate.*` | Reusable business logic | You (dbt) | Tables |
| **Marts** | `marts.*` | Final metrics for BI | You (dbt) | Tables |

---

## Raw Layer

### Purpose

Store data exactly as it comes from the source, with zero transformations.

### Schema Naming

- **Single-table sources** (CSV, simple APIs): `raw.<table>`
- **Multi-table sources** (Stripe, HubSpot): `raw_<source>.<table>`

### Examples

```sql
-- CSV source (orders.csv)
SELECT * FROM raw.orders;

-- Stripe source (multiple tables)
SELECT * FROM raw_stripe.charges;
SELECT * FROM raw_stripe.customers;
SELECT * FROM raw_stripe.subscriptions;

-- HubSpot source
SELECT * FROM raw_hubspot.contacts;
SELECT * FROM raw_hubspot.companies;
SELECT * FROM raw_hubspot.deals;
```

### dlt Metadata Columns

Every raw table includes dlt tracking columns:

| Column | Type | Purpose |
|--------|------|---------|
| `_dlt_load_id` | TEXT | Unique ID for each pipeline run |
| `_dlt_extracted_at` | TIMESTAMP | When dlt extracted this row |
| `_dlt_id` | TEXT | Unique row identifier |

Example:
```sql
SELECT
    id,
    customer_id,
    amount,
    _dlt_load_id,
    _dlt_extracted_at
FROM raw_stripe.charges
LIMIT 5;
```

### Immutability

Raw tables are **append-only** by default:
- Historical records are preserved
- Enables auditing and time-travel queries
- Deduplication happens in staging layer

---

## Staging Layer

### Purpose

Create clean, deduplicated views of raw data suitable for analysis.

### Auto-Generation

Staging models are auto-generated by Dango:

```bash
dango generate
```

This creates:
```
dbt/models/staging/
├── stg_stripe_charges.sql
├── stg_stripe_customers.sql
├── stg_stripe_subscriptions.sql
├── _stg_stripe__sources.yml      # Documents raw tables
└── _stg_stripe__schema.yml       # Column descriptions
```

### Deduplication Strategies

Configure in `.dango/sources.yml`:

```yaml
sources:
  - name: orders
    type: csv
    csv:
      deduplication_strategy: latest_only
      primary_key: order_id
      timestamp_column: updated_at
```

#### Available Strategies

| Strategy | Use Case | SQL Logic |
|----------|----------|-----------|
| `none` | Event logs, immutable data | No deduplication |
| `latest_only` | Snapshots, updated records | `ROW_NUMBER() OVER (PARTITION BY pk ORDER BY ts DESC)` |
| `append_only` | Already unique data | No deduplication |
| `scd_type2` | Track historical changes | Creates `valid_from`, `valid_to`, `is_current` columns |

### Example: `latest_only`

**Source config**:
```yaml
deduplication_strategy: latest_only
primary_key: customer_id
timestamp_column: updated_at
```

**Auto-generated SQL**:
```sql
-- stg_stripe_customers.sql (auto-generated)
{{ config(materialized='view', tags=['staging', 'stripe']) }}

WITH source_data AS (
    SELECT
        id,
        email,
        name,
        created,
        updated_at,
        _dlt_extracted_at
    FROM {{ source('raw_stripe', 'customers') }}
),

deduped AS (
    SELECT
        *,
        ROW_NUMBER() OVER (
            PARTITION BY id
            ORDER BY updated_at DESC
        ) AS rn
    FROM source_data
)

SELECT * EXCEPT(rn)
FROM deduped
WHERE rn = 1
```

### Example: SCD Type 2

**Source config**:
```yaml
deduplication_strategy: scd_type2
primary_key: product_id
timestamp_column: updated_at
```

**Auto-generated columns**:
```sql
SELECT
    product_id,
    name,
    price,
    valid_from,      -- When this version became active
    valid_to,        -- When this version was superseded
    is_current       -- Boolean: is this the latest version?
FROM staging.stg_products
WHERE product_id = 'prod_123';
```

Output:
```
product_id | name      | price | valid_from | valid_to   | is_current
-----------|-----------|-------|------------|------------|------------
prod_123   | Widget A  | 10.00 | 2024-01-01 | 2024-03-15 | false
prod_123   | Widget A  | 12.00 | 2024-03-15 | 2024-06-01 | false
prod_123   | Widget B  | 15.00 | 2024-06-01 | NULL       | true
```

### Materialization

Staging models are **views** (not tables) by default:
- Low storage overhead
- Always reflect latest raw data
- Fast to regenerate

---

## Intermediate Layer

### Purpose

Reusable business logic that combines multiple staging tables.

### User-Created

You write these models manually in `dbt/models/intermediate/`:

```sql
-- dbt/models/intermediate/int_customer_lifetime_value.sql
{{ config(materialized='table') }}

WITH charges AS (
    SELECT
        customer_id,
        SUM(amount) as total_spent,
        COUNT(*) as order_count,
        MIN(created) as first_order_date,
        MAX(created) as last_order_date
    FROM {{ ref('stg_stripe_charges') }}
    WHERE status = 'succeeded'
    GROUP BY customer_id
)

SELECT * FROM charges
```

### When to Use

Use intermediate models for:
- Complex joins across multiple tables
- Aggregations used in multiple marts
- Business logic shared across reports
- Performance optimization (pre-compute expensive operations)

### Example: Customer Segmentation

```sql
-- dbt/models/intermediate/int_customer_segments.sql
{{ config(materialized='table') }}

WITH ltv AS (
    SELECT * FROM {{ ref('int_customer_lifetime_value') }}
),

activity AS (
    SELECT
        customer_id,
        DATEDIFF('day', last_order_date, CURRENT_DATE) as days_since_last_order
    FROM ltv
)

SELECT
    customer_id,
    total_spent,
    order_count,
    days_since_last_order,
    CASE
        WHEN total_spent > 10000 THEN 'VIP'
        WHEN total_spent > 1000 THEN 'Premium'
        WHEN days_since_last_order > 90 THEN 'At Risk'
        ELSE 'Standard'
    END as customer_segment
FROM activity
```

---

## Marts Layer

### Purpose

Final, analytics-ready tables optimized for BI tools and reporting.

### User-Created

You write these models in `dbt/models/marts/`:

```sql
-- dbt/models/marts/customer_metrics.sql
{{ config(materialized='table') }}

WITH customers AS (
    SELECT * FROM {{ ref('stg_stripe_customers') }}
),

ltv AS (
    SELECT * FROM {{ ref('int_customer_lifetime_value') }}
),

segments AS (
    SELECT * FROM {{ ref('int_customer_segments') }}
)

SELECT
    c.id,
    c.email,
    c.name,
    c.created as customer_since,
    ltv.total_spent,
    ltv.order_count,
    ltv.first_order_date,
    ltv.last_order_date,
    segments.customer_segment,
    segments.days_since_last_order
FROM customers c
LEFT JOIN ltv ON c.id = ltv.customer_id
LEFT JOIN segments ON c.id = segments.customer_id
```

### Design Guidelines

**Denormalized for BI**:
- One wide table per business area
- Pre-joined dimensions
- Pre-calculated metrics
- Optimized for SELECT queries

**Materialized as tables**:
- Faster query performance in Metabase
- More storage, but worth it for end-users

**Examples of mart tables**:
- `customer_metrics` - Customer analytics
- `daily_sales` - Sales performance
- `product_performance` - Product analytics
- `churn_analysis` - Retention metrics

---

## Complete Example: Stripe Data Flow

Let's trace how Stripe payment data flows through all layers:

### 1. Raw Layer

**dlt loads data**:
```bash
dango sync --source stripe_payments
```

**Tables created**:
```sql
-- Raw charges
SELECT * FROM raw_stripe.charges LIMIT 3;
```

Output:
```
id          | customer_id | amount | currency | created    | _dlt_load_id
------------|-------------|--------|----------|------------|-------------
ch_001      | cus_A       | 1000   | usd      | 2024-01-15 | 1734567890
ch_001      | cus_A       | 1000   | usd      | 2024-01-15 | 1734567999
ch_002      | cus_B       | 2500   | usd      | 2024-01-16 | 1734567890
```

Note: Duplicate `ch_001` from multiple pipeline runs.

### 2. Staging Layer

**Generate staging models**:
```bash
dango generate
```

**Deduplication applied**:
```sql
SELECT * FROM staging.stg_stripe_charges LIMIT 3;
```

Output:
```
id     | customer_id | amount | currency | created
-------|-------------|--------|----------|------------
ch_001 | cus_A       | 1000   | usd      | 2024-01-15
ch_002 | cus_B       | 2500   | usd      | 2024-01-16
ch_003 | cus_A       | 500    | usd      | 2024-01-17
```

Duplicate removed, clean data.

### 3. Intermediate Layer

**Create reusable logic**:
```sql
-- dbt/models/intermediate/int_customer_ltv.sql
SELECT
    customer_id,
    SUM(amount) / 100.0 as total_spent_usd,
    COUNT(*) as purchase_count
FROM {{ ref('stg_stripe_charges') }}
GROUP BY customer_id
```

**Result**:
```sql
SELECT * FROM intermediate.int_customer_ltv;
```

Output:
```
customer_id | total_spent_usd | purchase_count
------------|-----------------|---------------
cus_A       | 15.00           | 2
cus_B       | 25.00           | 1
```

### 4. Marts Layer

**Build final metrics table**:
```sql
-- dbt/models/marts/customer_metrics.sql
WITH customers AS (
    SELECT * FROM {{ ref('stg_stripe_customers') }}
),

ltv AS (
    SELECT * FROM {{ ref('int_customer_ltv') }}
)

SELECT
    c.id,
    c.email,
    c.created as customer_since,
    COALESCE(ltv.total_spent_usd, 0) as lifetime_value,
    COALESCE(ltv.purchase_count, 0) as total_purchases
FROM customers c
LEFT JOIN ltv ON c.id = ltv.customer_id
```

**Query in Metabase**:
```sql
SELECT * FROM marts.customer_metrics
ORDER BY lifetime_value DESC
LIMIT 10;
```

Output:
```
id    | email          | customer_since | lifetime_value | total_purchases
------|----------------|----------------|----------------|----------------
cus_B | bob@acme.com   | 2024-01-10     | 25.00          | 1
cus_A | alice@corp.com | 2024-01-05     | 15.00          | 2
```

---

## Schema Evolution

### Adding New Columns

When source adds a new field, dlt auto-detects it:

**Before**:
```sql
SELECT id, email FROM raw_stripe.customers;
```

**After API update**:
```sql
SELECT id, email, phone FROM raw_stripe.customers;
-- 'phone' column automatically added by dlt
```

**Regenerate staging**:
```bash
dango generate
# Staging model updated to include 'phone'
```

### Handling Schema Changes

**Best practices**:
1. Run `dango sync` to get new columns in raw
2. Run `dango generate` to update staging models
3. Update intermediate/marts models manually if needed
4. Run `dango run` to apply transformations

---

## Querying Guidelines

### Query Staging or Marts?

| Use Case | Query Layer | Reason |
|----------|-------------|--------|
| Ad-hoc exploration | `staging.*` | Fast, simple, all columns |
| Production dashboards | `marts.*` | Optimized, pre-joined |
| Data quality checks | `raw.*` | See original source data |
| Debugging pipelines | `raw.*` + `_dlt_*` columns | Audit trail |

### Example: Debugging

```sql
-- Check if data is landing in raw
SELECT COUNT(*), MAX(_dlt_extracted_at)
FROM raw_stripe.charges;

-- Compare raw vs staging row counts
SELECT 'raw' as layer, COUNT(*) as rows
FROM raw_stripe.charges
UNION ALL
SELECT 'staging', COUNT(*)
FROM staging.stg_stripe_charges;
```

---

## Next Steps

- **[CLI Overview](cli-overview.md)** - Learn commands for managing data layers
- **[Project Structure](project-structure.md)** - See where these layers live in the file system
- **[Transformations](../transformations/index.md)** - Deep dive into dbt model development
